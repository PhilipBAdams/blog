<!doctype html><html lang=en-us>
<head><meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Results from a series of experiments using 'prior' data to improve PQ recall"><title>Hierarchical PQ for ANN Search</title>
<link rel=canonical href=https://blog.pbada.ms/p/multi-pq/>
<link rel=stylesheet href=/scss/style.min.css><meta property="og:title" content="Hierarchical PQ for ANN Search">
<meta property="og:description" content="Results from a series of experiments using 'prior' data to improve PQ recall">
<meta property="og:url" content="https://blog.pbada.ms/p/multi-pq/">
<meta property="og:site_name" content="Philip Adams">
<meta property="og:type" content="article"><meta property="article:section" content="Post"><meta property="article:tag" content="ANN Search"><meta property="article:tag" content="Quantization"><meta property="article:tag" content="Systems"><meta property="article:tag" content="C++"><meta property="article:published_time" content="2021-03-10T00:00:00+00:00"><meta property="article:modified_time" content="2021-03-10T00:00:00+00:00"><meta property="og:image" content="https://blog.pbada.ms/p/multi-pq/matryoshka-doll.jpg">
<meta name=twitter:title content="Hierarchical PQ for ANN Search">
<meta name=twitter:description content="Results from a series of experiments using 'prior' data to improve PQ recall"><meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://blog.pbada.ms/p/multi-pq/matryoshka-doll.jpg">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-6M07CQC6YP"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-6M07CQC6YP',{anonymize_ip:!1})}</script>
</head>
<body class="article-page has-toc">
<script>(function(){const a='StackColorScheme';localStorage.getItem(a)||localStorage.setItem(a,"light")})()</script><script>(function(){const b='StackColorScheme',a=localStorage.getItem(b),c=window.matchMedia('(prefers-color-scheme: dark)').matches===!0;a=='dark'||a==='auto'&&c?document.documentElement.dataset.scheme='dark':document.documentElement.dataset.scheme='light'})()</script>
<div class="container main-container flex
extended">
<div id=article-toolbar>
<a href=https://blog.pbada.ms class=back-home><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-chevron-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="15 6 9 12 15 18"/></svg>
<span>Back</span>
</a>
</div>
<main class="main full-width">
<article class="has-image main-article">
<header class=article-header>
<div class=article-image>
<a href=/p/multi-pq/>
<img src=/p/multi-pq/matryoshka-doll_hu157c719591c41da4bcb5ae3cb61cbe78_71596_800x0_resize_q75_box.jpg srcset="/p/multi-pq/matryoshka-doll_hu157c719591c41da4bcb5ae3cb61cbe78_71596_800x0_resize_q75_box.jpg 800w, /p/multi-pq/matryoshka-doll_hu157c719591c41da4bcb5ae3cb61cbe78_71596_1600x0_resize_q75_box.jpg 1600w" width=800 height=534 loading=lazy alt="Featured image of post Hierarchical PQ for ANN Search">
</a>
</div>
<div class=article-details>
<h2 class=article-title>
<a href=/p/multi-pq/>Hierarchical PQ for ANN Search</a>
</h2>
<h3 class=article-subtitle>
Results from a series of experiments using 'prior' data to improve PQ recall
</h3>
<footer class=article-time>
<div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Mar 10, 2021</time>
</div>
</footer>
</div>
</header>
<section class=article-content>
<h2 id=background>Background</h2>
<p>Approximate Nearest Neighbors (ANN) Search is a simple problem: given a vector query vector $q$, a set of database vectors $V$, and a metric $d$, find the $v\in V$ that minimize $d(q, v)$. The <em>approximate</em> part of this problem is that an algorithm is allowed to be wrong, and so in particular doesn&rsquo;t need to test every $v\in V$, allowing for significant improvements in performance through the use of various search structures and compression techniques.</p>
<p>An active research community has sprung up around building new techniques for this problem, driven by its importance to semantic search, where documents are embedded into a vector space to perform the search. <a class=link href=https://github.com/facebookresearch/faiss target=_blank rel=noopener>Facebook</a>, <a class=link href=https://github.com/google-research/google-research/tree/master/scann target=_blank rel=noopener>Google</a>, <a class=link href=https://github.com/microsoft/SPTAG target=_blank rel=noopener>Microsoft</a>, and <a class=link href=https://github.com/yahoojapan/NGT target=_blank rel=noopener>Yahoo</a> have all released productionized systems for ANN search, based on work done by research teams at each organization. There is even a site providing head-to-head benchmarks of different systems on reference datasets, at <a class=link href=http://ann-benchmarks.com target=_blank rel=noopener>ann-benchmarks.com</a>.</p>
<p>One well-known compression technique is Product Quantization (PQ), first applied to ANN search in 2010 by Jegou et al<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>. PQ is parameterized by two variables, $M$ and $nbits$. It splits each vector $v$ of dimension $d$ into $M$ subvectors $v_1,\dots, v_M$ of dimension $\frac{d}{M}$. Then, it trains $M$ codebooks with $2^{nbits}$ entries each using K-means clustering on the sets of subvectors. Now, every vector can be represented as a code of length $M\cdot nbits$. An additional benefit of a coding scheme is that rather than performing expensive distance calculations between vectors, a lookup table with $M\cdot 2^{nbits} \cdot 2^{nbits}$ entries can be constructed, and distance calculations are reduced to $M$ lookups in this table. Query vectors can be handled similarly, either by encoding the query vector under the PQ scheme and using the existing table (called Symmetric Distance Calculation (SDC)), or by computing a query-specific distance table of size $M \cdot nbits$ (called Asymmetric Distance Calculation (ADC)). For large datasets, these distance table approaches are significantly faster than normal distance calculations between unquantized vectors.</p>
<figure><img src=/img/pq_encode.gif alt="Process of encoding a vector with PQ"><figcaption>
<p>Process of encoding a vector with PQ</p>
</figcaption>
</figure>
<p>There have been a number of research attempts to improve on the performance of PQ. An early observation was that the standard PQ approach is reliant on the basis the data is presented in. For an extreme example, suppose that we are quantizing four-dimensional vectors with $M=2, nbits=1$, and the vectors are:</p>
<p>$$ V = \left\{ [1,0,1,0], [0,1,0,1], [1,1,1,1], [0,0,0,0] \right\}$$</p>
<p>In this case, we have that the first and third dimension are perfectly correlated, and similarly with the second and fourth, and can observe that if we were able to swap the second and third dimensions we would be able to quantize the vectors losslessly. Optimized PQ (OPQ), first presented by Ge et al. in 2013,<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> gives us a way to do that. Before training the codebooks for each subvector, we first train an orthogonal matrix, which is applied to all the vectors before they are quantized. This allows us &lsquo;waste&rsquo; fewer bits quantizing redundant information.</p>
<p>Another attempt to improve the recall-per-bit performance of PQ is Composite Quantization (CQ), presented by Zhang et al. in 2014<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>. In CQ, the training algorithm is given even more degrees of freedom to optimize with, as rather than splitting the vector into subvectors, it only requires that the subspaces spanned by the codebooks be mutually near-orthogonal, and reconstructs the resultant vector from codes through summation rather than concatenation of subvectors. This is a direct generalization of PQ to a larger configuration space, and the main contribution is the derivation of an efficient training method for the codebooks.</p>
<p>A final technique is Residual Quantization (RQ). This technique is applied after some initial quantization stage, either by means of a quantizer or a search structure like a K-means tree. The initial quantization stage trains a set of centroids and associates each vector with the nearest one, as usual. Then, the residual quantization stage involves quantizing the difference between the centroid and the actual datapoint. This second stage of quantization provides more accurate distance information between vectors, at the cost of space for a second set of codebooks and additional time encoding and decoding vectors. The process can be repeated multiple times, if desired, using the residuals from the previous stage at each step. It is used in Google&rsquo;s ScaNN vector search system<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>.</p>
<p>One thing that all of these attempts to improve PQ have in common is that they all attempt to make improvements solely from the information contained in the data vectors themselves, rather than from information about the distribution of queries that will be made against that data. They mainly accomplish this goal by finding new ways to expand the optimization space the trainer can explore without significantly increasing the cost of training. Improvements that can be made purely by looking at the data are more desirable, since we can be more confident in their robustness to different applications and/or query distributions. Additionally, there is already a well understood literature and set of benchmarks for evaluating the performance of purely data-dependent ANN approaches. However, this focus on query-independent approaches may leave some low-hanging fruit for application that exhibit stable query distributions, such as web search.</p>
<p>In web search, the distribution of queries is known to be long-tailed. Because of this phenomenon, if we can expend extra resources on encoding/searching for vectors that are NNs for queries that occur frequently, we would expect to get an outsized return on those resources. Additionally, since those vectors make up a small portion of the dataset, the cost is not significantly higher than a conventional technique and so we do not expect degradation in tail performance.</p>
<h3 id=definition-of-prior-data-and-hierarchical-pq>Definition of &lsquo;Prior&rsquo; data and Hierarchical PQ</h3>
<p>In the experiments discussed in this post, we consider two sets of queries, $Q_{\mathrm{train}}$ and $Q_{\mathrm{test}}$, drawn independently from the same distribution. We use the $Q_{\mathrm{train}}$ queries to compute the priors for each vector in the dataset. We define two types of prior, the linear and exponential falloff prior, as follows:</p>
<p>$$ \mathrm{NN}(q,v) = k : (v \text{ is a } k \text{-NN of } q \wedge \nexists k' &lt; k : v \text{ is a } k' \text{-NN of } q)$$
$$ p_\mathrm{linear}(Q,v,k) = \frac{1}{|Q|} \cdot \sum_{q\in Q} (\mathrm{NN}(q,v) \leq k)$$
$$ p_\mathrm{expfalloff}(Q,v) = \frac{1}{|Q|} \cdot \sum_{q\in Q} e^{-\mathrm{NN}(q,v)}$$</p>
<p>Essentially, the more likely the vector is to come up as a result in the searches on $Q_{\mathrm{train}}$, the higher the prior. We then conduct the actual searches using $Q_{\mathrm{test}}$, which was drawn independently.</p>
<p>What is the best way to use this prior information? We had a number of ideas, including using the prior data during the construction of the search structure to make high-prior vectors more likely to be &lsquo;touched&rsquo; by any given search, discussed further in the <a class=link href=#other-experiments>&lsquo;Other Experiments&rsquo;</a> section. We opted for a more straightforward approach, though: using more bits to encode vectors that are high-prior than those that are low-prior (and thus encoding high-prior vectors more accurately). This is the main idea behind the technique explored in the rest of this post, which we call <em>Hierarchical PQ</em> (HPQ). In HPQ, rather than having one codebook per subvector, we have a hierarchy of codebooks $C^0 \subset C^1 \subset \dots \subset C^n$. We can decide which codebook to quantize any given vector with according to the prior information. Crucially, because the codebooks contain each other, they are <em>compatible</em>: we can use a distance table built on $C^n$, the codebook at the top of the hierarchy, to compute the distance between two vectors quantized by different levels on the hierarchy. This means that when handling queries, we only have to build the ADC distance table once per query. Because the distance table is so frequently accessed during search, this has the additional benefit of better cache performance from spatial locality versus an approach with multiple disjoint codebooks.</p>
<p><figure class=gallery-image style=flex-grow:58;flex-basis:141px>
<a href=/p/multi-pq/hpq_cb.png data-size=615x1045>
<img src=/p/multi-pq/hpq_cb.png width=615 height=1045 srcset="/p/multi-pq/hpq_cb_huea04246045900be5e9c9e94affcdb0d4_16007_480x0_resize_box_3.png 480w, /p/multi-pq/hpq_cb_huea04246045900be5e9c9e94affcdb0d4_16007_1024x0_resize_box_3.png 1024w" loading=lazy alt="HPQ Codebook">
</a>
<figcaption>HPQ Codebook</figcaption>
</figure>
<figure class=gallery-image style=flex-grow:115;flex-basis:277px>
<a href=/p/multi-pq/hpq_dt.png data-size=1055x914>
<img src=/p/multi-pq/hpq_dt.png width=1055 height=914 srcset="/p/multi-pq/hpq_dt_hu555a4b975c80a9b96a92fe40d55229fc_39492_480x0_resize_box_3.png 480w, /p/multi-pq/hpq_dt_hu555a4b975c80a9b96a92fe40d55229fc_39492_1024x0_resize_box_3.png 1024w" loading=lazy alt="HPQ Distance Table">
</a>
<figcaption>HPQ Distance Table</figcaption>
</figure></p>
<h2 id=implementation-basics>Implementation Basics</h2>
<p>We based our implementation on FAISS<sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup>, a similarity search library from Facebook AI Research. FAISS has the advantages of good performance (including GPU support for common routines like PQ training), a convenient python interface for end-users generated with SWIG<sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup>, and fairly good extensibility thanks to nice pluggable interfaces for different components of the index.</p>
<p>Although it is theoretically possible to have an arbitrary number of levels of codebooks, it can become impractical to train many codebooks, and we expected to encounter diminishing returns to the number of tiers. So, we only implemented a two-level quantizer. Additionally, rather than modifying a K-means implementation to consider the &lsquo;fixed points&rsquo; from the lower level codebooks, we just ran a separate training stage for the higher level codebook.</p>
<h2 id=performance-characteristics>Performance Characteristics</h2>
<p>There are two significant performance issues with the HPQ approach. By far the most important one is the increase in distance table size. In order to compute distances between quantized vectors efficiently, we build distance tables that store the distances between the vectors represented by each pair of codes. The distance calculation is then reduced to $M$ lookups in this table. When we introduce new codebooks in the hierarchy, we have to expand the distance tables to include these codes. However, for the performance of these distance tables to be good, they need to be resident in cache. Introducing the new codebooks will often make them too large to fit, significantly increasing distance computation. As an example, a Skylake processor has 32 KiB/core of L1 data cache, 1 MiB/core of L2 cache, and 1.375 MiB/core of L3 cache. If we store distance entries as float, and have 32 8-bit codebooks, that takes exactly $32\times 2^8 \times 4 \text{ bytes} = 32 \times 2^{10} \text{ bytes} = 32 \text{ KiB}$, perfectly fitting in L1 cache. Adding 2 bits of extra codebook space for high-prior vectors increases the distance table size to $128 \text{KiB}$, causing evictions into L2. This can significantly affect performance.</p>
<p>Another performance factor to consider is that of non-byte-aligned operations. Normally, PQ uses a codebook size of 256 vectors (8 bits) because it is small enough to train efficiently, allows easy indexing into codebooks with byte types, and gives decent performance. The next byte-aligned size, 16 bits (65,536 vectors), is usually too large to train centroids for. So, we have to make a choice between wasting memory by storing 10 or 12 bit quantizers in 16 bits, or performing extra shifts and masking operations to pack the entries into only the required number of bits. This causes significant performance degradation, which is amplified by the performance offered by byte-aligned SIMD instructions.</p>
<h2 id=results>Results</h2>
<p>We ran experiments on a sampled set of 100M vectors sampled uniformly from the Deep1B dataset, provided by Babenko and Lempitsky in 2016<sup id=fnref:7><a href=#fn:7 class=footnote-ref role=doc-noteref>7</a></sup>. We assigned query priors according to a Zipf distribution. We then ran experiments comparing Recall@R for HNSW search, comparing normal PQ with HPQ. We ran these experiments with varying numbers of codebooks and codebook sizes, and also included a comparison of the &lsquo;Index Size Efficiency,&rsquo; or recall per megabyte of memory used by the index. We also compared performance to a simple ad-hoc approach of building a separate, higher quality index to store high-query-prior vectors.</p>
<p><figure class=gallery-image style=flex-grow:210;flex-basis:505px>
<a href=/p/multi-pq/hnsw_recall.png data-size=1064x505>
<img src=/p/multi-pq/hnsw_recall.png width=1064 height=505 srcset="/p/multi-pq/hnsw_recall_hub018a6a01496362e8dba160ba2bcb21b_89465_480x0_resize_box_3.png 480w, /p/multi-pq/hnsw_recall_hub018a6a01496362e8dba160ba2bcb21b_89465_1024x0_resize_box_3.png 1024w" loading=lazy alt="Recall@R of different Quantization approaches">
</a>
<figcaption>Recall@R of different Quantization approaches</figcaption>
</figure></p>
<p><figure class=gallery-image style=flex-grow:213;flex-basis:511px>
<a href=/p/multi-pq/size_efficiency.png data-size=1077x505>
<img src=/p/multi-pq/size_efficiency.png width=1077 height=505 srcset="/p/multi-pq/size_efficiency_hu081bd740cbf313af4289f1fee08f0c9e_88058_480x0_resize_box_3.png 480w, /p/multi-pq/size_efficiency_hu081bd740cbf313af4289f1fee08f0c9e_88058_1024x0_resize_box_3.png 1024w" loading=lazy alt="Size Efficiency of Quantization approaches">
</a>
<figcaption>Size Efficiency of Quantization approaches</figcaption>
</figure></p>
<p><figure class=gallery-image style=flex-grow:190;flex-basis:458px>
<a href=/p/multi-pq/perf_plot.png data-size=964x505>
<img src=/p/multi-pq/perf_plot.png width=964 height=505 srcset="/p/multi-pq/perf_plot_hua3530c4ba071ac4ee9bdd4634be2f4b9_78458_480x0_resize_box_3.png 480w, /p/multi-pq/perf_plot_hua3530c4ba071ac4ee9bdd4634be2f4b9_78458_1024x0_resize_box_3.png 1024w" loading=lazy alt="Performance (QPS) of Quantization approaches with varying search depth">
</a>
<figcaption>Performance (QPS) of Quantization approaches with varying search depth</figcaption>
</figure></p>
<p>The main positive result that we see is good size efficiency on the low end. Though MULTIPQ_32x4_10 (That&rsquo;s 32 codebooks, 4 bits per codebook for low-prior vectors and 10 bits for high-prior ones) takes up barely any more space in memory than PQ_32x4, it has significantly higher recall. This could prove useful in extremely memory-constrained scenarios. However, at a more typical 8 bits per codebook, the recall advantage, though still present, is smaller. We also see the performance penalty of larger codebooks and distance tables that don&rsquo;t fit as well in cache, with significantly reduced QPS for the MULTIPQ implementations. All the approaches perform much worse at high R than simply building two indexes, although that approach can cause resource challenges.</p>
<h2 id=other-experiments>Other Experiments</h2>
<p>We tried two other techniques for &lsquo;prior-aware&rsquo; search. The first was to modify the search structure in order to make high-query-prior vectors more likely to be considered during the search routine. We implemented this idea in a search structure called Hierarchical Navigable Small Worlds (HNSW), first presented by Malkov et al.<sup id=fnref:8><a href=#fn:8 class=footnote-ref role=doc-noteref>8</a></sup> in 2018. HNSW works by creating multiple layers, and selecting which layer each vector should have as it&rsquo;s highest layer by sampling from an exponential distribution. So, the top layer will have very few vectors, and the lowest layer contains the whole dataset. Within each layer, a neighborhood graph is constructed, allowing for search within the layer. A full search starts at the top layer, searches within it, then goes down into the next layer, searches within that layer, and so on until the bottom layer is reached. This technique achieves some of the best results on <a class=link href=http://ann-benchmarks.com target=_blank rel=noopener>ann-benchmarks.com</a>. We implemented a modification where instead of selecting a vector&rsquo;s layer by sampling from a random distribution, it selected based on the sample plus the prior. This did have the effect of pushing high-prior vectors to higher layers, meaning that they were seen by more searches. However, in our experiments, that structural change did not have any impact on performance. Investigating this further may be the topic of a future blog post!</p>
<p>Another technique we tried was based around &lsquo;learned termination&rsquo;. Typical ANN search systems take some static parameter, usually called the &lsquo;search depth&rsquo;, that determines how extensive a search to perform (essentially, how many vectors to look at). Previous work by Ge et al in 2020<sup id=fnref:9><a href=#fn:9 class=footnote-ref role=doc-noteref>9</a></sup> used gradient boosting decision trees to predict midway through a search how helpful searching further would be, allowing early termination of the search when the model predicted that better results were unlikely to be found from searching further. This allows significant improvements in search throughput. We modified this system by including the query-priors of search candidates as features to the decision trees. If a search had no high-query-prior results, it might try searching a little bit further, to see if it can find one. Again, we didn&rsquo;t find any significant change in search performance or accuracy from this change.</p>
<h2 id=acknowledgments>Acknowledgments</h2>
<p>The majority of the work on this project was done as part of a course at UChicago, CMSC 33550, taught/advised by <a class=link href=https://raulcastrofernandez.com/ target=_blank rel=noopener>Raul Castro Fernandez</a>. My partner for the project was my friend <a class=link href=https://github.com/akabdo target=_blank rel=noopener>Abdo</a>.</p>
<h2 id=errata>Errata</h2>
<ul>
<li>The HPQ distance table figure contains off-by-one errors. Entries that read $2^{nbits_k}$ should read $2^{nbits_k} -1$.</li>
</ul>
<hr>
<p>Note: the <a class=link href=https://foto.wuestenigel.com/opening-matryoshka-dolls/ target=_blank rel=noopener>featured image for this post</a> is by Marco Verch, and is licensed under <a class=link href=https://creativecommons.org/licenses/by/2.0/ target=_blank rel=noopener>CC BY 2.0</a>.</p>
<section class=footnotes role=doc-endnotes>
<hr>
<ol>
<li id=fn:1 role=doc-endnote>
<p>Jegou, Herve, Matthijs Douze, and Cordelia Schmid. &ldquo;Product quantization for nearest neighbor search.&rdquo; <em>IEEE transactions on pattern analysis and machine intelligence</em> 33, no. 1 (2010): 117-128.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p>
</li>
<li id=fn:2 role=doc-endnote>
<p>Ge, Tiezheng, Kaiming He, Qifa Ke, and Jian Sun. &ldquo;Optimized product quantization.&rdquo; <em>IEEE transactions on pattern analysis and machine intelligence</em> 36, no. 4 (2013): 744-755.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p>
</li>
<li id=fn:3 role=doc-endnote>
<p>Zhang, Ting, Chao Du, and Jingdong Wang. &ldquo;Composite quantization for approximate nearest neighbor search.&rdquo; In <em>International Conference on Machine Learning</em>, pp. 838-846. PMLR, 2014.&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p>
</li>
<li id=fn:4 role=doc-endnote>
<p>Guo, Ruiqi, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, and Sanjiv Kumar. &ldquo;Accelerating large-scale inference with anisotropic vector quantization.&rdquo; In International Conference on Machine Learning, pp. 3887-3896. PMLR, 2020.&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p>
</li>
<li id=fn:5 role=doc-endnote>
<p><a class=link href=https://github.com/facebookresearch/faiss target=_blank rel=noopener>https://github.com/facebookresearch/faiss</a>&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p>
</li>
<li id=fn:6 role=doc-endnote>
<p><a class=link href=http://www.swig.org/ target=_blank rel=noopener>http://www.swig.org/</a>&#160;<a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p>
</li>
<li id=fn:7 role=doc-endnote>
<p>Babenko, Artem, and Victor Lempitsky. &ldquo;Efficient indexing of billion-scale datasets of deep descriptors.&rdquo; In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2055-2063. 2016.&#160;<a href=#fnref:7 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p>
</li>
<li id=fn:8 role=doc-endnote>
<p>Malkov, Yu A., and Dmitry A. Yashunin. &ldquo;Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs.&rdquo; <em>IEEE transactions on pattern analysis and machine intelligence</em> 42, no. 4 (2018): 824-836.&#160;<a href=#fnref:8 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p>
</li>
<li id=fn:9 role=doc-endnote>
<p>Li, Conglong, Minjia Zhang, David G. Andersen, and Yuxiong He. &ldquo;Improving approximate nearest neighbor search through learned adaptive early termination.&rdquo; In Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data, pp. 2539-2554. 2020.&#160;<a href=#fnref:9 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p>
</li>
</ol>
</section>
</section>
<footer class=article-footer>
<section class=article-tags>
<a href=/tags/ann-search/>ANN Search</a>
<a href=/tags/quantization/>Quantization</a>
<a href=/tags/systems/>Systems</a>
<a href=/tags/c++/>C++</a>
</section>
<section class=article-copyright><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span>
</section>
</footer>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.css integrity=sha384-RZU/ijkSsFbcmivfdRBQDtwuwVqK7GMOw6IMvKyeWL2K5UAlyp6WonmB8m7Jd0Hn crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.js integrity=sha384-pK1WpvzWVBQiP0/GjnvRxV4mOb0oxFuyRxJlk6vVw146n3egcN5C925NCP7a7BY8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/contrib/auto-render.min.js integrity=sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{renderMathInElement(document.querySelector(`.article-content`),{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})})</script>
</article>
<aside class=related-contents--wrapper>
</aside>
<footer class=site-footer>
<section class=copyright>
&copy;
2021 Philip Adams
</section>
<section class=powerby>
Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> <br>
Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.2.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a>
</section>
</footer>
<div class=pswp tabindex=-1 role=dialog aria-hidden=true>
<div class=pswp__bg></div>
<div class=pswp__scroll-wrap>
<div class=pswp__container>
<div class=pswp__item></div>
<div class=pswp__item></div>
<div class=pswp__item></div>
</div>
<div class="pswp__ui pswp__ui--hidden">
<div class=pswp__top-bar>
<div class=pswp__counter></div>
<button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
<div class=pswp__preloader>
<div class=pswp__preloader__icn>
<div class=pswp__preloader__cut>
<div class=pswp__preloader__donut></div>
</div>
</div>
</div>
</div>
<div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
<div class=pswp__share-tooltip></div>
</div>
<button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
</button>
<div class=pswp__caption>
<div class=pswp__caption__center></div>
</div>
</div>
</div>
</div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.css integrity="sha256-c0uckgykQ9v5k+IqViZOZKc47Jn7KQil4/MP3ySA3F8=" crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.css integrity="sha256-SBLU4vv6CA6lHsZ1XyTdhyjJxCjPif/TRkjnsyGAGnE=" crossorigin=anonymous>
</main>
<aside class="sidebar right-sidebar sticky">
<section class="widget archives">
<div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg>
</div>
<h2 class="widget-title section-title">Table of contents</h2>
<div class=widget--toc>
<nav id=TableOfContents>
<ul>
<li><a href=#background>Background</a>
<ul>
<li><a href=#definition-of-prior-data-and-hierarchical-pq>Definition of &lsquo;Prior&rsquo; data and Hierarchical PQ</a></li>
</ul>
</li>
<li><a href=#implementation-basics>Implementation Basics</a></li>
<li><a href=#performance-characteristics>Performance Characteristics</a></li>
<li><a href=#results>Results</a></li>
<li><a href=#other-experiments>Other Experiments</a></li>
<li><a href=#acknowledgments>Acknowledgments</a></li>
<li><a href=#errata>Errata</a></li>
</ul>
</nav>
</div>
</section>
</aside>
</div>
<script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.5/dist/vibrant.min.js integrity="sha256-5NovOZc4iwiAWTYIFiIM7DxKUXKWvpVEuMEPLzcm5/g=" crossorigin=anonymous defer></script><script type=text/javascript src=/ts/main.js defer></script>
<script>(function(){const a=document.createElement('link');a.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",a.type="text/css",a.rel="stylesheet",document.head.appendChild(a)})()</script>
</body>
</html>